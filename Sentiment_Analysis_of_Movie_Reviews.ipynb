{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Adit Rada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose, you want to watch a movie a friend suggested in the weekend. You go to IMDb website and look at the review of the movie. The problem is that after a long day, you don't have the patience to read a legnthy review! Would it not be nice to just know if the review is positive or negative without reading the whole thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will try build a model that when we input a movie review, we can get an answer if it is a positive review or a negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset of movie reviews from the IMDb website collected by Andrew Maas. \n",
    "This dataset contains the text of the reviews, together with a label that indicates\n",
    "whether a review is “positive” or “negative.” This is a two-class classification dataset where reviews with a score of 6 or higher are labeled as positive, and the rest as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided as text files in two separate folders,\n",
    "one for the training data and one for the test data. Each of these in turn has two subfolders,\n",
    "one called pos and one called neg. The pos folder contains all the positive reviews, each as a separate text file, and similarly for the neg folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the helper function in scikit-learn to load files stored\n",
    "in such a folder structure, called load_files. We apply the load_files function first to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"aclImdb/train/\")\n",
    "# Load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n"
     ]
    }
   ],
   "source": [
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one movie review as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_train is a list of length 25,000, where each entry is a string\n",
    "containing a review. We printed the review with index 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the review contains some HTML line breaks (<b.r /.>); it is better to clean the data and remove this formatting before we proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was collected such that the positive class and the negative class balanced,\n",
    "so that there are as many positive as negative strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test dataset in the same manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOAL: The task we want to solve is as follows: given a review, we want to assign the label “positive” or “negative” based on the text content of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is a standard binary classification task. However, the text data is not in a format that a learning algorithim can handle. We need to convert the string representation of the text into a numeric representation that we can apply our learning algorithms to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Text Data as a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this representaiton, we only count how often each word appears in each text in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps to this proocess:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Tokenization. Split each document into the words that appear in it (called tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Vocabulary building. Collect a vocabulary of all words that appear in any of the documents, and number them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Encoding. For each document, count how often each of the words in the vocabulary appear in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The bag-of-words representation is implemented in CountVectorizer in scikit-learn, which is a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words representation is stored in a SciPy sparse matrix that only stores\n",
    "the entries that are nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect1 = CountVectorizer().fit(text_train)\n",
    "X_train1 = vect1.transform(text_train)\n",
    "\n",
    "print(\"X_train:\\n{}\".format(repr(X_train1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of X_train, the bag-of-words representation of the training data, is\n",
    "25,000×74,849, indicating that the vocabulary contains 74,849 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the vocabulary in a bit more detail.\n",
    "To access the vocabulary, we use the get_feature_name method of the\n",
    "vectorizer, which returns a convenient list where each entry corresponds to one feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "First 20 features:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Features 20010 to 20030:\n",
      "['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Every 2000th feature:\n",
      "['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect1.get_feature_names()\n",
    "\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"--\"*60)\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"--\"*60)\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"--\"*60)\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking  in the vocabulary, we find a collection\n",
    "of English words starting with “dra”. You might notice that for \"draught\",\n",
    "\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\n",
    "vocabulary as distinct words. These words have very closely related semantic meanings,\n",
    "and counting them as different words, corresponding to different features,\n",
    "might not be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to improve our feature extraction, let’s obtain a quantitative measure of\n",
    "performance by actually building a classifier. We have the training labels stored in\n",
    "y_train and the bag-of-words representation of the training data in X_train, so we\n",
    "can train a classifier on this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high-dimensional, sparse data like this, linear\n",
    "models like LogisticRegression often work best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - (trial using original data); Test score - 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=10000), X_train1, y_train, cv=5)\n",
    "\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that LogisticRegression\n",
    "has a regularization hyperparameter, C, which we can tune via cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}    # Hyperparameter values to search over\n",
    "\n",
    "grid1 = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
    "grid1.fit(X_train1, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid1.best_score_))\n",
    "print(\"Best parameters: \", grid1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a cross-validation score of 89% using C=0.1. We can now assess the generalization\n",
    "performance of this parameter setting on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "X_test1 = vect1.transform(text_test)\n",
    "print(\"{:.2f}\".format(grid1.score(X_test1, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The generalization score of model 1 is 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This trial model actually does quite well! Let us see if we can do any better with feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - (after feature extraction of setting min_df); Test score - 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer\n",
    "extracts tokens using a regular expression. By default, the regular expression that is\n",
    "used is \"\\b\\w\\w+\\b\". This means it\n",
    "finds all sequences of characters that consist of at least two letters or numbers (\\w)\n",
    "and that are separated by word boundaries (\\b). It does not find single-letter words,\n",
    "and it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single\n",
    "word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer also converts all words to lowercase characters, so that\n",
    "“soon”, “Soon”, and “sOon” all correspond to the same token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way toreduce the massive number of features is to only use tokens that appear in at least two documents or any other number. A token that appears only in a single document is unlikely to appear in the test\n",
    "set and is therefore not helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  We can set the minimum number of documents a token needs to appear in with the min_df parameter. Let us set it to 5 movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect2 = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train2 = vect2.transform(text_train)\n",
    "\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By requiring at least five appearances of each token, we can bring down the number\n",
    "of features to 27,271 (all the way from 74849!) Almost 2/3 of the features are removed. This will make the model drastically faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of doing this is that any mis-spellings are removed, as it is very unlikely that the same word will be incorrectly spellled by 5 different and independent reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if the model perfomance is still similar after removing almost 2/3 of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n"
     ]
    }
   ],
   "source": [
    "grid2 = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
    "grid2.fit(X_train2, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid2.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best validation score is still 89% with so many less features and the time taken to train the model was almost twice as quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "X_test2 = vect2.transform(text_test)\n",
    "print(\"{:.2f}\".format(grid2.score(X_test2, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The generalization score of model 2 is 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note : If the transform method of CountVectorizer is called on a document that contains words that were not contained in the training data, these words will be ignored as they are not part of the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way that we can get rid of uninformative words is by discarding words that\n",
    "are too frequent to be informative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches: using a languagespecific\n",
    "list of stopwords, or discarding words that appear too frequently. scikitlearn\n",
    "has a built-in list of English stopwords in the feature_extraction.text\n",
    "module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['take', 'into', 'someone', 'but', 'your', 'ten', 'yourselves', 'interest', 'top', 'during', 'you', 'him', 'neither', 'up', 'them', 'around', 'across', 'amongst', 'part', 'this', 'yourself', 'cry', 'else', 'me', 'afterwards', 'elsewhere', 'move', 'thin', 'please', 'against', 'bill', 'why']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 318 words, it might not make too big of a difference. Let us try it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - (using Stopwords); Test score - 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Specifying stop_words=\"english\" uses the built-in list.\n",
    "# We could also augment it and pass our own.\n",
    "vect3 = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train3 = vect3.transform(text_train)\n",
    "\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 305 (27,271–26,966) fewer features in the dataset, which means that\n",
    "most, but not all, of the stopwords appeared. Let’s run the grid search again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88\n"
     ]
    }
   ],
   "source": [
    "grid3 = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
    "grid3.fit(X_train3, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid3.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search perfomance did decrease, but only slightly. Since only 305 features were removed, the model perfomance did not increase too. So removing stopwords many not be worth the hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n"
     ]
    }
   ],
   "source": [
    "X_test3 = vect3.transform(text_test)\n",
    "print(\"{:.2f}\".format(grid3.score(X_test3, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The generalization score of model 3 is 87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling Data with tf–idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of dropping features that seem to be important, another approach is to give weighting the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A common way to do this is using the term frequency–inverse document frequency (tf–idf) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this method is to give high weight to any term that appears\n",
    "often in a particular document, but not in many documents in the corpus. If a word\n",
    "appears often in a particular document, but not in very many documents, it is likely\n",
    "to be very descriptive of the content of that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what the intuition between the individual terms:\n",
    "Suppose we have a collection of documents and we want rank them based on the query \"the brown dog\"\n",
    "\n",
    "###### Term Frequency\n",
    "First we collect all documnets where those three words appear by counting the number of times each word appears in the document. This is the term frequency; the higher the count, the more relevant the documnet might be.\n",
    "\n",
    "######  Inverse document frequency\n",
    "Because the term \"the\" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms \"brown\" and \"dog\".\n",
    "\n",
    "Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set as a whole and increases the weight of terms that occur rarely in all the documnets.\n",
    "\n",
    "\n",
    "So, we can think about it like this: if the word \"dog\" for example has a high tf-idf for a particulat document, this means that the word appears frequently inside that document and very less frequently in other doucumnets. So this document has to have a high ranking in the search results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How will this help our classification task?\n",
    "The simple way to think how this will help us is that if a particular word e.g \"Avengers\" has a high tf-idf score and say it has a positive review label in the test set. Then, if a similar word with has a similarly high tf-idf score is found in the during prediction, then maybe, that review is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn implements the\n",
    "tf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix\n",
    "output produced by CountVectorizer and transforms it, and TfidfVectorizer,\n",
    "which takes in the text data and does both the bag-of-words feature extraction and\n",
    "the tf–idf transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because tf–idf actually makes use of the statistical properties of the training data, we\n",
    "will use a pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - (using tf-idf); Test score - 89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Make the pipeline\n",
    "pipe4 = make_pipeline(TfidfVectorizer(min_df=5, norm=None),LogisticRegression(max_iter=10000))\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Do the grid search and find the best model\n",
    "grid4 = GridSearchCV(pipe4, param_grid, cv=5, n_jobs=-1)\n",
    "grid4.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid4.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect which words tf–idf found most important. Keep in mind\n",
    "that the tf–idf scaling is meant to find words that distinguish documents, but it is a\n",
    "purely unsupervised technique. So, “important” here does not necessarily relate to the\n",
    "“positive review” and “negative review” labels we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'occurred'\n",
      " 'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'fond'\n",
      " 'stinker' 'avoided' 'emphasis' 'commented' 'disappoint' 'realizing'\n",
      " 'downhill' 'inane']\n",
      "Features with highest tfidf: \n",
      "['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\n",
      " 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\n",
      " 'khouri' 'zizek' 'rob' 'timon' 'titanic']\n"
     ]
    }
   ],
   "source": [
    "# First, we extract the TfidfVectorizer from the pipeline:\n",
    "vectorizer4 = grid4.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "\n",
    "# Transform the training dataset\n",
    "X_train4 = vectorizer4.transform(text_train)\n",
    "\n",
    "# Find maximum value for each of the features over the dataset\n",
    "max_value = X_train4.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = np.array(vectorizer4.get_feature_names())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf: \\n{}\".format(feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with low tf–idf are those that either are very commonly used across documents\n",
    "or are only used sparingly, and only in very long documents.\n",
    "Features with high tf-idf are those that appear frequently in a particular document and less frequently accross the whole set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the words that have low inverse document frequency—that is, those\n",
    "that appear frequently and are therefore deemed less important. The inverse document\n",
    "frequency values found on the training set are stored in the idf_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer4.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these are mostly English stopwords like \"the\" and \"no\". But some are\n",
    "clearly domain-specific to the movie reviews, like \"movie\", \"film\", \"time\", \"story\",\n",
    "and so on. Interestingly, \"good\", \"great\", and \"bad\" are also among the most frequent\n",
    "and therefore “least relevant” words according to the tf–idf measure, even\n",
    "though we might expect these to be very important for our sentiment analysis task, the learning algorithim does not make use of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n"
     ]
    }
   ],
   "source": [
    "logreg4 = grid4.best_estimator_.named_steps[\"logisticregression\"]\n",
    "\n",
    "X_test4 = vectorizer4.transform(text_test)\n",
    "print(\"{:.2f}\".format(logreg4.score(X_test4, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The generalization score of model 4 is 89%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words with n-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One of the main disadvantages of using a bag-of-words representation is that word order is completely discarded and context is lost.\n",
    " Therefore, the two strings “it’s bad, not good at all” and “it’s good, not bad at all” have exactly the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is a way of capturing context when\n",
    "using a bag-of-words representation, by not only considering the counts of single\n",
    "tokens, but also the counts of pairs or triplets of tokens that appear next to each other.\n",
    "Pairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\n",
    "more generally sequences of tokens are known as n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the range of tokens that are considered as features by changing the ngram_range hyperparameter of\n",
    "CountVectorizer or TfidfVectorizer. \n",
    "\n",
    "ngram_range is a tuple, consisting of the minimum length and the maximum length of the sequences of tokens that are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default is to consider only single tokens i.e. unigrams so the default is (1,1)\n",
    "If we pass (2,2), then only bigrams are considered. If we pass (1,3), then unigrams, bigrams and trigrams are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try this for our dataset and see if there is any improvement in perfomance. We will use grid search to see if adding bigrams or trigrams makes any difference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - (using bigrams); Test score - 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.91\n",
      "Best parameters:\n",
      "{'logisticregression__C': 100, 'tfidfvectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "pipe5 = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=10000))\n",
    "\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 100],\n",
    "                \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2)]}\n",
    "\n",
    "grid5 = GridSearchCV(pipe5, param_grid, cv=5, n_jobs=-1)\n",
    "grid5.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid5.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid5.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see, we have crossed the coveted 90% mark by adding bigrams, though at the cost of using more computing power and time. There are now almost 155 thousand features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement in perfomance might have come due to inclusion of words like \"definitely worth\", \"well worth\", \"so good\" and so on. These provide more context and give much more information about the label then if they were considered as unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing all the models we have built with small updates and improvements to each, we can see that using the bigrams model is the best when we get the result from GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how it generalizes by testing it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rada_\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1874: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if copy != \"deprecated\":\n",
      "C:\\Users\\rada_\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1878: FutureWarning: 'copy' param is unused and has been deprecated since version 0.22. Backward compatibility for 'copy' will be removed in 0.24.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90\n"
     ]
    }
   ],
   "source": [
    "# First, we extract the TfidfVectorizer from the pipeline:\n",
    "vectorizer5 = grid5.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "\n",
    "# Transform the test dataset\n",
    "X_test5 = vectorizer5.transform(text_test, y_test)\n",
    "\n",
    "# Retirve the logistic regression model from the pipeline\n",
    "logreg5 = grid5.best_estimator_.named_steps[\"logisticregression\"]\n",
    "\n",
    "# Perform the test to check generalization\n",
    "print(\"{:.2f}\".format(logreg5.score(X_test5, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The generalization score of model 5 is 90%. We will use this as the final model for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go back to the motivation behind the model that started this. Remeber the weekend situation? Where you don't want to read the whole long review but just want to see in it is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how thim model works on an Avatar review (my favourite movie!) taken randomly from the internet.\n",
    "\n",
    "It is pretty long and all we want to see is if it is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "avatar_review = [b\"I saw this epic last night at the Empire Leicester Sq in London, which is a superb venue in which to view this film. Huge screen, excellent sound and an extraordinary Dolby, 3 dimensional image. The whole effect is mind blowing. This is a Must see movie, innovative, and extraordinary. I think it will be regarded by most cinema goers as another milestone in the history of the art. The level of realism achieved is remarkable, and although the film is relatively long in real time, it retains it's excitement and holds the audience's attention to the end. Performances are good, but this is not the sort of film that dwells on big star value for the actors, although Sigorney Weaver does shine and delivers a very convincing performance, as do the rest of the cast. But as there is so much entertainment and action value on screen the human element does not dominate in the usual way. As Writer/Director, James Cameron deserves high praise for this creation and in my opinion it will break box office records. I thoroughly enjoyed this film.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We transform the text review so that the learning algorithim can use it for prediction\n",
    "avatar_test = vectorizer5.transform(avatar_review)\n",
    "\n",
    "# We now predict the outcome\n",
    "logreg5.predict(avatar_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 is positive; 0 is negative. This is again a binary classification problem remember."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horray!!! The model did predicts that this review is positive, we have now saved all that time we would have spent reading a long review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citiation:\n",
    "The dataset can be found at https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
